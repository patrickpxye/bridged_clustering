{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/PatrickYe/miniforge3/envs/bkm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "1     211\n",
      "6     150\n",
      "5     129\n",
      "9     113\n",
      "26     86\n",
      "2      76\n",
      "7      50\n",
      "12     49\n",
      "8      48\n",
      "13     39\n",
      "4      39\n",
      "29     38\n",
      "11     31\n",
      "31     29\n",
      "3      28\n",
      "30     27\n",
      "14     26\n",
      "23     25\n",
      "24     22\n",
      "16     22\n",
      "21     21\n",
      "10     21\n",
      "33     20\n",
      "18     17\n",
      "32     15\n",
      "15     14\n",
      "27     13\n",
      "25     13\n",
      "19     13\n",
      "28     13\n",
      "35     13\n",
      "22     12\n",
      "37     12\n",
      "34     12\n",
      "20     12\n",
      "17     12\n",
      "0      12\n",
      "36     10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_mutual_info_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sacrebleu.metrics import CHRF\n",
    "from k_means_constrained import KMeansConstrained\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import gaussian_kde\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv(\"data/wiki_df.csv\")\n",
    "df['x'] = df['x'].apply(lambda x: np.fromstring(x[1:-1], sep=',')).tolist()  # Convert string back to numpy array\n",
    "df['yv'] = df['yv'].apply(lambda x: np.fromstring(x[1:-1], sep=',')).tolist()  # Convert string back to numpy array\n",
    "df['zv'] = df['zv'].apply(lambda x: np.fromstring(x[1:-1], sep=',')).tolist()  # Convert string back to numpy array\n",
    "X = np.vstack(df['zv'].values)  # shape: (n_samples, embedding_dim)\n",
    "db = DBSCAN(eps=0.36, min_samples=12, metric='cosine').fit(X)\n",
    "labels = db.labels_\n",
    "df = df.assign(cluster=labels)\n",
    "df_valid = df[df['cluster'] != -1].copy()\n",
    "cluster_sizes = df_valid['cluster'].value_counts()\n",
    "eligible = cluster_sizes[cluster_sizes >= 12].index\n",
    "print(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# define a set of stopwords to ignore\n",
    "stop_words = {\n",
    "    'on','in','of','to','for','with','a','an','the','and','or','but',\n",
    "    'is','are','be','as','by','at','from','that','this','these','those',\n",
    "    # add more as needed...\n",
    "}\n",
    "\n",
    "pruned_clusters = []\n",
    "for cl in eligible:\n",
    "    sub = df_valid[df_valid['cluster'] == cl]\n",
    "    # split z into words, lowercase, filter out stopwords & non-alpha tokens\n",
    "    word_lists = sub['z'].str.split(',').apply(\n",
    "        lambda shards: [\n",
    "            w.lower()\n",
    "            for shard in shards\n",
    "            for w in shard.strip().split()\n",
    "            if w.isalpha() and w.lower() not in stop_words\n",
    "        ]\n",
    "    )\n",
    "    # count only the filtered words\n",
    "    word_counts = Counter(w for words in word_lists for w in words)\n",
    "    if not word_counts:\n",
    "        # no valid words in this cluster\n",
    "        continue\n",
    "\n",
    "    most_common_word, count = word_counts.most_common(1)[0]\n",
    "    print(f\"Most common non-stopword in cluster {cl}: {most_common_word} (count: {count})\")\n",
    "\n",
    "    # keep only rows containing that word\n",
    "    mask = word_lists.apply(lambda words: most_common_word in words)\n",
    "    pruned = sub[mask]\n",
    "    if len(pruned) >= 12:\n",
    "        pruned_clusters.append(pruned)\n",
    "\n",
    "# concatenate and re-compute eligibility\n",
    "df_pruned       = pd.concat(pruned_clusters, ignore_index=True)\n",
    "pruned_counts   = df_pruned['cluster'].value_counts()\n",
    "eligible_pruned = pruned_counts[pruned_counts >= 30].index\n",
    "eligible_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible clusters:\n",
      "cluster\n",
      "2     432\n",
      "6     331\n",
      "1     278\n",
      "9     136\n",
      "3     106\n",
      "31     90\n",
      "33     72\n",
      "10     56\n",
      "77     55\n",
      "46     53\n",
      "22     43\n",
      "66     40\n",
      "0      37\n",
      "30     36\n",
      "27     35\n",
      "13     34\n",
      "36     34\n",
      "42     34\n",
      "26     32\n",
      "16     30\n",
      "32     30\n",
      "20     27\n",
      "75     26\n",
      "91     25\n",
      "57     25\n",
      "60     25\n",
      "65     25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "from k_means_constrained import KMeansConstrained\n",
    "\n",
    "\n",
    "### Columns of this dataset:\n",
    "# - 'image_emb': image embeddings\n",
    "# - 'caption': text captions\n",
    "# - 'caption_emb': text embeddings\n",
    "# - 'img_id': unique image identifier\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Load & initial TF–IDF + DBSCAN on captions to get “clusters” for sampling\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "df = pd.read_parquet(\"data/flickr30k.parquet\")\n",
    "df = df.reset_index(drop=True)\n",
    "df.drop(columns=['sentids', 'split','filename'], inplace=True)\n",
    "df['_pair'] = df.apply(lambda r: list(zip(r['caption'], r['caption_embs'])), axis=1)\n",
    "df = df.explode('_pair').reset_index(drop=True)\n",
    "df[['caption', 'caption_emb']] = pd.DataFrame(df['_pair'].tolist(), index=df.index)\n",
    "df = df.drop(columns=['caption_embs', '_pair'])\n",
    "# df = df[:100000]\n",
    "\n",
    "def tfidf_encode(captions, max_df=0.9, min_df=3, stop_words='english'):\n",
    "    vect = TfidfVectorizer(max_df=max_df, min_df=min_df,\n",
    "                           stop_words=stop_words)\n",
    "    X = vect.fit_transform(captions)\n",
    "    return X, vect\n",
    "\n",
    "X_tfidf, vect = tfidf_encode(df['caption'])\n",
    "db = DBSCAN(eps=0.6, min_samples=12, metric='euclidean')\n",
    "df['cluster'] = db.fit_predict(X_tfidf)\n",
    "\n",
    "df_valid     = df[df['cluster'] != -1].copy()\n",
    "cluster_sz   = df_valid['cluster'].value_counts()\n",
    "eligible     = cluster_sz[cluster_sz >= 25].index\n",
    "# eligible = eligible[2:]\n",
    "print(\"Eligible clusters:\")\n",
    "print(cluster_sz.loc[eligible].sort_values(ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bkm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
